{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyltr.metrics import NDCG\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from scipy.special import softmax\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = pd.read_parquet('train_visits.parquet.gzip')\n",
    "ports = pd.read_parquet('ports_prep.parquet.gzip')\n",
    "vessels = pd.read_parquet('vessels_prep.parquet.gzip')\n",
    "\n",
    "ports = ports.set_index('port_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take subsample to reduce memory\n",
    "- Drop irrelevant columns (e.g. datetimes)\n",
    "- Add target\n",
    "- Add option (needed for creating the dataset)\n",
    "- Create an identifier since a vessel can have multiple visits in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = visits[visits['target_port_index'].notnull()].sample(25000).sort_values('entry_datetime')\n",
    "\n",
    "df = df.drop(['entry_datetime', 'target_entry_datetime', \n",
    "                      'exit_datetime', 'previous_exit_datetime',\n",
    "                      'target_stay_duration', 'target_travel_duration',\n",
    "                      'previous_portname', 'previous_entry_datetime', \n",
    "                      'prev2_exit_datetime'], 1)\n",
    "\n",
    "df['target'] = 1\n",
    "df['option_id'] = 0\n",
    "\n",
    "df['identifier'] = df['mmsi'].astype(str) + '_' + df.groupby('mmsi').cumcount().astype('str')\n",
    "df = df.set_index('identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for ranking\n",
    "- Get pivots; creates pivot tables with new features\n",
    "- Create dataset; creates a dataset for ranking purposes. Each instance is expanded to 75 instances each with a different potential target port.\n",
    "- add features; adds new features (also from the pivots) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pivots(port_only=False):\n",
    "    #add iso3 codes to hist_visits\n",
    "    ports=pd.read_parquet('ports_prep.parquet.gzip').set_index('port_index')\n",
    "    hist_visits = pd.read_parquet('hist_visits.parquet.gzip')\n",
    "    hist_visits = pd.merge(hist_visits, ports[['iso3']].reset_index(), how='left', on='port_index')\n",
    "    hist_visits = pd.merge(hist_visits, ports[['iso3']].rename(columns={'iso3':'target_iso3'}), \n",
    "                               how='left', left_on='target_port_index', right_index=True)\n",
    "    \n",
    "    hist_visits['hist_connections'] = 1\n",
    "    \n",
    "    #Connections between ports\n",
    "    port_pt = pd.pivot_table(hist_visits, index=['port_index', 'target_port_index'], \n",
    "                        values='hist_connections', aggfunc='count').fillna(0).reset_index()\n",
    "    port_pt.rename(columns={'hist_connections': 'port_hist_connections'}, inplace=True)\n",
    "    port_pt = port_pt.set_index(['port_index', 'target_port_index'])\n",
    "    \n",
    "    if port_only:\n",
    "        return port_pt\n",
    "    \n",
    "    #travel time/stay duration between ports\n",
    "    port_pt2 = pd.pivot_table(hist_visits, index=['port_index', 'target_port_index'], \n",
    "                        values=['target_travel_duration', 'target_stay_duration'], \n",
    "                         aggfunc=['mean', 'std', 'max', 'min']).reset_index()\n",
    "    port_pt2 = port_pt2.set_index(['port_index', 'target_port_index'])\n",
    "    port_pt2.columns = ['ports_' + '_'.join((str(j), str(k))) for j, k in port_pt2.columns]  \n",
    "    \n",
    "    \n",
    "    #connections between countries\n",
    "    iso_pt = pd.pivot_table(hist_visits, index=['iso3', 'target_iso3'], \n",
    "                        values='hist_connections', aggfunc='count').fillna(0).reset_index()\n",
    "    iso_pt.rename(columns={'hist_connections': 'iso_hist_connections'}, inplace=True)\n",
    "    iso_pt = iso_pt.set_index(['iso3', 'target_iso3'])\n",
    "\n",
    "    #travel time/stay duration between countries\n",
    "    iso_pt2 = pd.pivot_table(hist_visits, index=['iso3', 'target_iso3'], \n",
    "                        values=['target_travel_duration', 'target_stay_duration'], \n",
    "                         aggfunc=['mean', 'std', 'max', 'min']).reset_index()\n",
    "    iso_pt2 = iso_pt2.set_index(['iso3', 'target_iso3'])\n",
    "    iso_pt2.columns = ['iso3_' + '_'.join((str(j), str(k))) for j, k in iso_pt2.columns]    \n",
    "    \n",
    "\n",
    "    #merge datasets\n",
    "    port_pt = pd.merge(port_pt, port_pt2, left_index=True, right_index=True)\n",
    "    iso_pt = pd.merge(iso_pt, iso_pt2, left_index=True, right_index=True)\n",
    "\n",
    "    \n",
    "    #previous port --> target port connections\n",
    "    prev_pt = pd.pivot_table(hist_visits, index=['previous_port_index', 'target_port_index'], \n",
    "                        values='hist_connections', aggfunc='count').fillna(0).reset_index()\n",
    "    prev_pt.rename(columns={'hist_connections': 'prev_hist_connections'}, inplace=True)\n",
    "    prev_pt = prev_pt.set_index(['previous_port_index', 'target_port_index'])\n",
    "    \n",
    "    #prev2 port --> target port connections\n",
    "    prev2_pt = pd.pivot_table(hist_visits, index=['prev2_port_index', 'target_port_index'], \n",
    "                            values='hist_connections', aggfunc='count').fillna(0).reset_index()\n",
    "    prev2_pt.rename(columns={'hist_connections': 'prev2_hist_connections'}, inplace=True)\n",
    "    prev2_pt = prev2_pt.set_index(['prev2_port_index', 'target_port_index'])\n",
    "    \n",
    "    return port_pt, iso_pt, prev_pt, prev2_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, n_regular_connections=10, n_random_ports=40):\n",
    "    port_pt = get_pivots(port_only=True)\n",
    "    \n",
    "    #first get some regular connections\n",
    "    dataset1 = pd.DataFrame(df[['port_index']])\n",
    "    dataset1 = pd.merge(dataset1.reset_index(), port_pt[['port_hist_connections']].reset_index(), how='left', on='port_index')\n",
    "    dataset1 = dataset1.sort_values(['identifier', 'port_hist_connections'], ascending=[True, False])\n",
    "    dataset1 = dataset1.groupby('identifier').head(n_regular_connections)\n",
    "    dataset1 = dataset1.drop(['port_index', 'port_hist_connections'], 1)\n",
    "\n",
    "    #then get some random ports as possible targets\n",
    "    dataset = pd.concat([pd.DataFrame(index=df.index)]*n_random_ports)\n",
    "    dataset['target_port_index'] = np.random.choice(ports.index.astype(int).values, len(dataset), replace=True)\n",
    "\n",
    "    #add them together, sort on identifier, set the target to 0 and give them an option id, and set the index\n",
    "    dataset = pd.concat([dataset1, dataset.reset_index()], sort=False).reset_index(drop=True)\n",
    "    dataset = dataset.sort_values('identifier')\n",
    "    dataset['option_id'] = dataset.groupby('identifier').cumcount()+1\n",
    "    dataset['target'] = 0\n",
    "    dataset = dataset.set_index('identifier')\n",
    "\n",
    "    #add the true targets to the dataset, merge the original data with the dataset\n",
    "    dataset = pd.concat([dataset, df[dataset.columns]])\n",
    "    dataset = pd.merge(df.drop(['option_id', 'target', 'target_port_index'], 1), \n",
    "                        dataset, how='right', left_index=True, right_index=True)\n",
    "\n",
    "    #set the index and drop duplicates\n",
    "    dataset = dataset.reset_index()\n",
    "    dataset = dataset.drop_duplicates(subset=['identifier', 'port_index', 'target_port_index'], keep='last')\n",
    "\n",
    "    #drop option_id\n",
    "    dataset = dataset.drop('option_id', 1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dataset):\n",
    "    port_pt, iso_pt, prev_pt, prev2_pt = get_pivots()\n",
    "    ports = pd.read_parquet('ports_prep.parquet.gzip').set_index('port_index')\n",
    "    distances = pd.read_excel('CERDI.xlsx').set_index(['iso1', 'iso2'])[['seadistance']]\n",
    "    og_port_columns = ports.columns\n",
    "    \n",
    "    #add target port features\n",
    "    ports.columns = ['target_' + column for column in og_port_columns]\n",
    "    dataset = pd.merge(dataset, ports, how='left', left_on='target_port_index', right_index=True)\n",
    "    ports.columns = og_port_columns\n",
    "    \n",
    "    #add port_pt, iso_pt features\n",
    "    dataset = pd.merge(dataset, port_pt, how='left', left_on=['port_index', 'target_port_index'], right_index=True)\n",
    "    dataset = pd.merge(dataset, iso_pt, how='left', left_on=['iso3', 'target_iso3'], right_index=True)\n",
    "    dataset['port_hist_connections'] = dataset['port_hist_connections'].fillna(0)\n",
    "    dataset['iso_hist_connections'] = dataset['iso_hist_connections'].fillna(0)\n",
    "    \n",
    "    #add fav port connections\n",
    "    port_pt = port_pt.reset_index().groupby('port_index').head(1)\n",
    "    dataset = pd.merge(dataset, port_pt[['port_index', 'target_port_index', \n",
    "                       'port_hist_connections']].rename(columns={'target_port_index': 'port_fav_port',\n",
    "                       'port_hist_connections': 'port_fav_port_count'}), \n",
    "                       how='left', on='port_index')\n",
    "    dataset['port_fav_port_count'] = dataset['port_fav_port_count'].fillna(0)\n",
    "    dataset['port_fav_port'] = dataset['port_fav_port'].astype('object').fillna('Unknown').astype('category')\n",
    "\n",
    "    ports.columns = ['port_fav_' + column for column in og_port_columns]\n",
    "    dataset = pd.merge(dataset, ports[['port_fav_port_lat', 'port_fav_port_long']], \n",
    "                       how='left', left_on='target_port_index', right_index=True)\n",
    "    ports.columns = og_port_columns\n",
    "    \n",
    "    #add previous and prev2 pt features\n",
    "    dataset = pd.merge(dataset, prev_pt, how='left', left_on=['previous_port_index', 'target_port_index'], right_index=True)\n",
    "    dataset = pd.merge(dataset, prev2_pt, how='left', left_on=['prev2_port_index', 'target_port_index'], right_index=True)\n",
    "    dataset['prev_hist_connections'] = dataset['prev_hist_connections'].fillna(0)\n",
    "    dataset['prev2_hist_connections'] = dataset['prev2_hist_connections'].fillna(0)    \n",
    "    \n",
    "    #add seadistance curr -- target\n",
    "    dataset = pd.merge(dataset, distances[['seadistance']], how='left',\n",
    "                       left_on=['iso3', 'target_iso3'], right_index=True)\n",
    "    dataset['seadistance'] = dataset['seadistance'].fillna(0)\n",
    "\n",
    "    #add seadistance prev -- target\n",
    "    dataset = pd.merge(dataset, distances[['seadistance']].rename(columns={'seadistance': 'prev_tar_seadistance'}), \n",
    "                       how='left', left_on=['previous_iso3', 'target_iso3'], right_index=True)\n",
    "    dataset['prev_tar_seadistance'] = dataset['prev_tar_seadistance'].fillna(0)\n",
    "\n",
    "    #add seadistance prev2 -- target\n",
    "    dataset = pd.merge(dataset, distances[['seadistance']].rename(columns={'seadistance': 'prev2_tar_seadistance'}), \n",
    "                       how='left', left_on=['prev2_iso3', 'target_iso3'], right_index=True)\n",
    "    dataset['prev2_tar_seadistance'] = dataset['prev2_tar_seadistance'].fillna(0)\n",
    "    \n",
    "    #add euclidean distances\n",
    "    dataset['eucl_cur_tar'] = np.sqrt((dataset['port_lat'] - dataset['target_port_lat'])**2 + \n",
    "                                 (dataset['port_long'] - dataset['target_port_long'])**2)\n",
    "    dataset['eucl_cur_prev'] = np.sqrt((dataset['port_lat'] - dataset['previous_port_lat'])**2 + \n",
    "                                 (dataset['port_long'] - dataset['previous_port_long'])**2)\n",
    "    dataset['eucl_tar_prev'] = np.sqrt((dataset['target_port_lat'] - dataset['previous_port_lat'])**2 + \n",
    "                                 (dataset['target_port_long'] - dataset['previous_port_long'])**2)\n",
    "    dataset['eucl_ves_tar'] = np.sqrt((dataset['mean_port_lat'] - dataset['target_port_lat'])**2 + \n",
    "                                 (dataset['mean_port_long'] - dataset['target_port_long'])**2)\n",
    "    dataset['eucl_ves_cur'] = np.sqrt((dataset['mean_port_lat'] - dataset['port_lat'])**2 + \n",
    "                                 (dataset['mean_port_long'] - dataset['port_long'])**2)\n",
    "    dataset['eucl_ves_prev'] = np.sqrt((dataset['mean_port_lat'] - dataset['previous_port_lat'])**2 + \n",
    "                                 (dataset['mean_port_long'] - dataset['previous_port_long'])**2)\n",
    "    \n",
    "    dataset['eucl_cur_prev2'] = np.sqrt((dataset['port_lat'] - dataset['prev2_port_lat'])**2 + \n",
    "                                 (dataset['port_long'] - dataset['prev2_port_long'])**2)\n",
    "    dataset['eucl_tar_prev2'] = np.sqrt((dataset['target_port_lat'] - dataset['prev2_port_lat'])**2 + \n",
    "                                 (dataset['target_port_long'] - dataset['prev2_port_long'])**2)\n",
    "    dataset['eucl_prev_prev2'] = np.sqrt((dataset['previous_port_lat'] - dataset['prev2_port_lat'])**2 + \n",
    "                                 (dataset['previous_port_long'] - dataset['prev2_port_long'])**2)\n",
    "    dataset['eucl_ves_prev2'] = np.sqrt((dataset['mean_port_lat'] - dataset['prev2_port_lat'])**2 + \n",
    "                                 (dataset['mean_port_long'] - dataset['prev2_port_long'])**2)    \n",
    "\n",
    "    dataset['eucl_ves_fav_cur'] = np.sqrt((dataset['vessel_fav_port_lat'] - dataset['port_lat'])**2 + \n",
    "                                 (dataset['vessel_fav_port_long'] - dataset['port_long'])**2)\n",
    "    dataset['eucl_ves_fav_tar'] = np.sqrt((dataset['vessel_fav_port_lat'] - dataset['target_port_lat'])**2 + \n",
    "                                 (dataset['vessel_fav_port_long'] - dataset['target_port_long'])**2)\n",
    "    dataset['eucl_ves_fav_prev'] = np.sqrt((dataset['vessel_fav_port_lat'] - dataset['previous_port_lat'])**2 + \n",
    "                                 (dataset['vessel_fav_port_long'] - dataset['previous_port_long'])**2)\n",
    "    dataset['eucl_ves_fav_prev2'] = np.sqrt((dataset['vessel_fav_port_lat'] - dataset['prev2_port_lat'])**2 + \n",
    "                                 (dataset['vessel_fav_port_long'] - dataset['prev2_port_long'])**2) \n",
    "    dataset['eucl_ves_fav_ves'] = np.sqrt((dataset['vessel_fav_port_lat'] - dataset['mean_port_lat'])**2 + \n",
    "                                 (dataset['vessel_fav_port_long'] - dataset['mean_port_long'])**2)\n",
    "    \n",
    "    dataset['eucl_port_fav_tar'] = np.sqrt((dataset['port_fav_port_lat'] - dataset['target_port_lat'])**2 + \n",
    "                                 (dataset['port_fav_port_long'] - dataset['target_port_long'])**2)\n",
    "    dataset['eucl_port_fav_prev'] = np.sqrt((dataset['port_fav_port_lat'] - dataset['previous_port_lat'])**2 + \n",
    "                                 (dataset['port_fav_port_long'] - dataset['previous_port_long'])**2)\n",
    "    dataset['eucl_port_fav_ves'] = np.sqrt((dataset['port_fav_port_lat'] - dataset['mean_port_lat'])**2 + \n",
    "                                 (dataset['port_fav_port_long'] - dataset['mean_port_long'])**2)    \n",
    "    dataset['eucl_port_fav_ves_fav'] = np.sqrt((dataset['port_fav_port_lat'] - dataset['vessel_fav_port_lat'])**2 + \n",
    "                                 (dataset['port_fav_port_long'] - dataset['vessel_fav_port_long'])**2)   \n",
    "    \n",
    "    #add rankings\n",
    "    dataset['rank_port_connections'] = dataset.groupby('identifier')['port_hist_connections'].rank(axis=0, \n",
    "                                                                                        method='min', ascending=False)\n",
    "    dataset['rank_iso_connections'] = dataset.groupby('identifier')['iso_hist_connections'].rank(axis=0, \n",
    "                                                                                        method='min', ascending=False)\n",
    "    dataset['rank_prev_connections'] = dataset.groupby('identifier')['prev_hist_connections'].rank(axis=0, \n",
    "                                                                                        method='min', ascending=False)\n",
    "    dataset['rank_prev2_connections'] = dataset.groupby('identifier')['prev2_hist_connections'].rank(axis=0, \n",
    "                                                                                        method='min', ascending=False)\n",
    "    dataset['rank_seadistance'] = dataset.groupby('identifier')['seadistance'].rank(axis=0, method='min')\n",
    "    dataset['rank_prev_tar_seadistance'] = dataset.groupby('identifier')['prev_tar_seadistance'].rank(axis=0, method='min')\n",
    "    dataset['rank_eucl_cur_tar'] = dataset.groupby('identifier')['eucl_cur_tar'].rank(axis=0, method='min')\n",
    "    dataset['rank_eucl_tar_prev'] = dataset.groupby('identifier')['eucl_tar_prev'].rank(axis=0, method='min')\n",
    "    dataset['rank_eucl_tar_ves_fav'] = dataset.groupby('identifier')['eucl_ves_fav_tar'].rank(axis=0, method='min')\n",
    "    dataset['rank_eucl_tar_mean'] = dataset.groupby('identifier')['eucl_ves_tar'].rank(axis=0, method='min')\n",
    "    dataset['rank_eucl_port_fav_tar'] = dataset.groupby('identifier')['eucl_port_fav_tar'].rank(axis=0, method='min')\n",
    "    \n",
    "    #rename a column\n",
    "    dataset.rename(columns={'n_visits_y': 'n_visits'}, inplace=True)\n",
    "    \n",
    "    #add some difference metrics\n",
    "    for col in ['port_lat', 'port_long', 'n_visits', 'n_unique_vessels',\n",
    "           'n_high_speed', 'n_medium_speed', 'n_Chemical/Oil Tanker', 'n_Container Ship', 'n_Crude Oil Tanker',\n",
    "           'n_General Cargo Ship', 'n_Tanker', 'n_large_length', 'n_medium_length',\n",
    "           'n_small_length', 'n_very large_length', 'n_large_depth',\n",
    "           'n_medium_depth', 'n_small_depth', 'n_very large_depth', 'port_avg_distance_to_port', 'port_avg_travel_duration',\n",
    "           'port_avg_stay_duration' ]:\n",
    "        dataset[f'prev_curr_diff_{col}'] = abs(dataset[col] - dataset[f'previous_{col}'])\n",
    "        dataset[f'curr_targ_diff_{col}'] = abs(dataset[f'target_{col}'] - dataset[col])\n",
    "        dataset[f'fav_targ_diff_{col}'] = abs(dataset[f'target_{col}'] - dataset[f'vessel_fav_{col}'])   \n",
    "    \n",
    "    \n",
    "    dataset['prev_speed_coor'] = (dataset['eucl_cur_prev']*111) / np.exp(dataset['previous_travel_duration'])\n",
    "    dataset['prev_speed_sead'] = dataset['previous_seadistance'] / np.exp(dataset['previous_travel_duration'])\n",
    "    dataset['prev2_speed_coor'] = (dataset['eucl_prev_prev2']*111) / np.exp(dataset['prev2_travel_duration'])\n",
    "    dataset['prev2_speed_sead'] = dataset['prev2_prev_seadistance'] / np.exp(dataset['prev2_travel_duration'])\n",
    "    \n",
    "    dataset['exp_trav_prev_coor'] = np.log((dataset['eucl_cur_tar']*111) / dataset['prev_speed_coor'])\n",
    "    dataset['exp_trav_prev_sead'] = np.log(dataset['seadistance'] / dataset['prev_speed_sead'])\n",
    "    dataset['exp_trav_prev2_coor'] = np.log((dataset['eucl_cur_tar']*111) / dataset['prev2_speed_coor'])\n",
    "    dataset['exp_trav_prev2_sead'] = np.log(dataset['seadistance'] / dataset['prev2_speed_sead'])\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(df, n_regular_connections=25, n_random_ports=50)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = add_features(dataset)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings\n",
    "- Mappings are import to make sure that the DSS prediction uses the same mapping between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {}\n",
    "\n",
    "for column in dataset.columns[dataset.dtypes == 'object']:\n",
    "    if (column != 'identifier') and ('iso3' not in column) and ('portname' not in column):\n",
    "        print(column)\n",
    "        i = 0\n",
    "        colmap = {}\n",
    "        for value in dataset[column].unique():\n",
    "            colmap[value] = i\n",
    "            i+=1\n",
    "        \n",
    "        dataset[column] = dataset[column].map(colmap).astype('category')\n",
    "\n",
    "        mappings[column] = colmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mappings, open('mappings.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT\n",
    "- Split the data on time\n",
    "- There are some columns we do not want to use in the model. For example using the mmsi prohibits the model to be effective for unseen vessels. Also the features with many categories (such as mmsi) could induce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.set_index(['identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srch_ids = np.asarray(dataset.index.get_level_values(0).unique())\n",
    "\n",
    "\n",
    "train_cutoff = int(len(srch_ids)*0.6)\n",
    "val_cutoff = int(len(srch_ids)*0.8)\n",
    "\n",
    "\n",
    "dataset['count'] = 1\n",
    "traindf = dataset[dataset.index.get_level_values(0).isin(srch_ids[:train_cutoff])]\n",
    "trainids = pd.pivot_table(traindf, index='identifier', values='count', aggfunc='count')['count'].values\n",
    "\n",
    "valdf = dataset[dataset.index.get_level_values(0).isin(srch_ids[train_cutoff:val_cutoff])]\n",
    "valids = pd.pivot_table(valdf, index='identifier', values='count', aggfunc='count')['count'].values\n",
    "\n",
    "testdf = dataset[dataset.index.get_level_values(0).isin(srch_ids[val_cutoff:])]\n",
    "testids = pd.pivot_table(valdf, index='identifier', values='count', aggfunc='count')['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_train = ['count', 'target', 'mmsi', 'portname', 'target_portname', 'prev2_portname',\n",
    "                 'mmsi_iso3', 'iso3', 'previous_iso3', 'prev2_iso3', 'target_iso3', 'target_port_index',\n",
    "                 'port_index', 'prev2_port_index', 'vessel_fav_portname', 'vessel_fav_iso3', 'vessel_fav_port_index',\n",
    "                 'port_fav_port']\n",
    "\n",
    "cols_train = traindf.drop(cols_not_train, 1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION\n",
    "- Define a model\n",
    "- Fit a model\n",
    "- Select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRanker(n_estimators=500, learning_rate=.05, verbose=1,\n",
    "                   min_child_samples=5000, lambdarank_truncation_level=1,\n",
    "                   max_depth=7, num_leaves=15,\n",
    "                   metric='ndcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(traindf[cols_train], traindf['target'].values, group=trainids, \n",
    "         eval_set=[(traindf[cols_train], traindf['target'].values), \n",
    "                   (valdf[cols_train], valdf['target'].values)],\n",
    "         eval_group=[trainids, valids],\n",
    "         eval_metric='ndcg', eval_at=1,\n",
    "         verbose=10, early_stopping_rounds=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = pd.DataFrame({'Feature': cols_train, \n",
    "                         'Importance': model.feature_importances_}).sort_values('Importance').tail(50)['Feature'].values\n",
    "\n",
    "pd.DataFrame({'Feature': cols_train, 'Importance': model.feature_importances_}).sort_values('Importance').tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "- Define a model\n",
    "- Train a model on the selected features\n",
    "- Check performance\n",
    "- Save the used features and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRanker(n_estimators=2500, learning_rate=.025, verbose=1,\n",
    "                   min_child_samples=25000, lambdarank_truncation_level=1,\n",
    "                   max_depth=4, num_leaves=10,\n",
    "                   metric='ndcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(traindf[new_cols], traindf['target'].values, group=trainids, \n",
    "         eval_set=[(traindf[new_cols], traindf['target'].values), \n",
    "                   (valdf[new_cols], valdf['target'].values)],\n",
    "         eval_group=[trainids, valids],\n",
    "         eval_metric='ndcg', eval_at=1,\n",
    "         verbose=10, early_stopping_rounds=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = NDCG(k=1)\n",
    "\n",
    "prediction = model.predict(testdf[new_cols])\n",
    "\n",
    "print ('Random ranking:', metric.calc_mean_random(testdf.index.get_level_values(0).values, \n",
    "                                                  testdf['target'].values))\n",
    "\n",
    "print ('Our model:', metric.calc_mean(testdf.index.get_level_values(0).values, \n",
    "                                      testdf['target'].values, prediction))\n",
    "\n",
    "#prediction_train = model.predict(x_train)\n",
    "#print ('Train model:', metric.calc_mean(np.asarray(qids_train), np.asarray(y_train), prediction_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Feature': new_cols, 'Importance': model.feature_importances_}).sort_values('Importance').tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgb.plot_metric(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_cols, open('port_cols.p', 'wb'))\n",
    "pickle.dump(model, open('port_model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank complete port dataset\n",
    "- Since the previous test set only contained 75 ports per instance it is not representative of the actual environment in which it will be used\n",
    "- Therefore the performance is evaluated on a part of the test set where for each instance all ports are possible targets\n",
    "- This is the final performance of the model\n",
    "- Lastly save the predictions to be able to analyze them in the results notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.index.get_level_values(0).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(dataset, samples=1750, test=True):\n",
    "    if test:\n",
    "        random_ids = np.random.choice(testdf.index.get_level_values(0).unique(), samples, replace=False)\n",
    "    else:\n",
    "        random_ids = np.random.choice(traindf.index.get_level_values(0).unique(), samples, replace=False)\n",
    "\n",
    "\n",
    "    data = dataset[(dataset.index.get_level_values(0).isin(random_ids)) & \n",
    "                   (dataset['target'] == 1)][[col for col in dataset.columns \n",
    "                                              if (col in visits.columns) or (('n_visits' in col) and \n",
    "                                                                            ('target' not in col))]]\n",
    "\n",
    "    true_ports = data.reset_index()[['identifier', 'target_port_index']]\n",
    "\n",
    "    return data.reset_index().drop('target_port_index', 1), true_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(data, mappings):\n",
    "    samples = len(data)\n",
    "    data = data.iloc[np.repeat(np.arange(len(data)), len(ports))]\n",
    "    data['target_port_index'] = np.tile(ports.index.astype('int').values, samples)\n",
    "    \n",
    "    #data = data[data['port_index'] != data['target_port_index']]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, true_ports = get_subset(dataset, samples=1500, test=True)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_test_dataset(data, mappings)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_features(data)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns[data.dtypes == 'object']:\n",
    "    if (column != 'identifier') and ('iso3' not in column) and ('portname' not in column):\n",
    "        print(column)\n",
    "        data[column] = data[column].map(mappings[column]).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prediction'] = model.predict(data[new_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data[['identifier', 'port_index', 'target_port_index', 'prediction']].sort_values(['identifier', 'prediction'], \n",
    "                        ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "results['rank'] = results.groupby('identifier').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ports['target'] = 1\n",
    "results = pd.merge(results, true_ports, how='left', on=['identifier', 'target_port_index']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('port_prediction_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "- Baseline uses the historic visits as well as the train dataset\n",
    "- Computes the most occuring connections and predicts those\n",
    "- Save the predictions to evaluate them in the Results notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_visits = pd.read_parquet('hist_visits.parquet.gzip')\n",
    "hist_visits = hist_visits[['port_index', 'target_port_index']]\n",
    "hist_visits = pd.concat([hist_visits, traindf[['port_index', 'target_port_index']]])\n",
    "hist_visits['connections'] = 1\n",
    "\n",
    "print(hist_visits.shape)\n",
    "hist_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pd.pivot_table(hist_visits, index=['port_index', 'target_port_index'], \n",
    "                    values='connections', aggfunc='count').fillna(0).reset_index()\n",
    "\n",
    "pt = pt.sort_values(['port_index', 'connections'], ascending=[True, False])\n",
    "\n",
    "print(pt.shape)\n",
    "pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbase = testdf[testdf['target'] == 1][['mmsi', 'port_index', 'target_port_index', 'target']].reset_index()\n",
    "testbase = testbase.drop_duplicates('mmsi').drop('mmsi', 1)\n",
    "\n",
    "pt = pd.merge(testbase[['identifier', 'port_index']], pt, how='left', on='port_index')\n",
    "\n",
    "testbase = pd.merge(pt, testbase, \n",
    "         how='left', on=['identifier', 'port_index', 'target_port_index'])\n",
    "\n",
    "testbase['target'] = testbase['target'].fillna(0)\n",
    "testbase['rank'] = testbase.groupby('identifier').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbase.to_csv('BASELINE_port_prediction_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
