{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING 3 DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ports\n",
    "- Fill iso3 missing values first with the iso3_op\n",
    "- Fill other missing values with unknown; might want to manually fill iso3 missing values left after first step\n",
    "- Drop the irrelevant columns\n",
    "    - Drop latitude/longitude since the correct values are in the visits dataset\n",
    "- Drop the instance with no information\n",
    "- Recode some stuff, to be able to determine natuical distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ports():\n",
    "    ports = pd.read_csv('ports.csv', delimiter=';', index_col='port_index', encoding='latin-1')\n",
    "    print('old shape:', ports.shape)\n",
    "\n",
    "    ports['iso3'] = ports['iso3'].fillna(ports['iso3_op'])\n",
    "    ports[['prttype', 'prtsize', 'status', 'iso3']] = ports[['prttype', 'prtsize', 'status', 'iso3']].fillna('Unknown')\n",
    "    ports = ports.drop(['code', 'maxdepth', 'maxlength', 'annualcapa', 'country', \n",
    "                        'iso3_op', 'latitude', 'longitude'], 1).reset_index()\n",
    "    ports = ports[ports['port_index'].notnull()]\n",
    "\n",
    "    ports = ports.replace({'Sea Port': 'Sea',\n",
    "                          'Restricted': 'Unknown',\n",
    "                          'Planned': 'Unknown',\n",
    "                          'COG COD': 'COG',          # Both congo\n",
    "                          'MOZ, MWI, ZWE' : 'MOZ',   # Mozambique\n",
    "                          'MOZ, ZWE': 'MOZ',         # Mozambique\n",
    "                          'ESH': 'MAR',              # Western Sahara --> Marocco\n",
    "                          'JEY': 'FRA',              # Jersey --> France\n",
    "                          'IMY': 'TUR'})             # Milyan language? --> Turkey\n",
    "\n",
    "    ports.loc[ports['portname'] == 'Duqm', 'iso3'] = 'OMN' #the only port that we dont have iso3, but does occur in visits\n",
    "\n",
    "    print('new shape:', ports.shape)\n",
    "    return ports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vessels\n",
    "- Compute a linear regression for length and depth with the instances that have both length and depth\n",
    "- Predict and fill the values for the instances that have depth/length missing\n",
    "- Fill the last 3 instances (which neither have length nor depth) with the mean length and depth\n",
    "- Drop the predictions\n",
    "- Categorize length/depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_vessels():\n",
    "    vessels = pd.read_csv('vessels_subset.csv', delimiter=';')\n",
    "    print('old shape:', vessels.shape)\n",
    "    vessels = vessels.drop('imo', 1)\n",
    "\n",
    "    #select data\n",
    "    data = vessels[vessels['length'].notnull() & vessels['depth'].notnull()]\n",
    "    length_data = vessels[vessels['depth'].notnull()]\n",
    "    depth_data = vessels[vessels['length'].notnull()]\n",
    "\n",
    "    #linear regressions\n",
    "    lmod = LinearRegression()\n",
    "    lmod.fit(data[['depth']], data['length'])\n",
    "    length_data['length_pred'] = lmod.predict(length_data[['depth']])\n",
    "\n",
    "    dmod = LinearRegression()\n",
    "    dmod.fit(data[['length']], data['depth'])\n",
    "    depth_data['depth_pred'] = dmod.predict(depth_data[['length']])\n",
    "\n",
    "    #fill predictions\n",
    "    vessels = pd.merge(vessels, depth_data[['mmsi', 'depth_pred']], how='left', on='mmsi')\n",
    "    vessels = pd.merge(vessels, length_data[['mmsi', 'length_pred']], how='left', on='mmsi')\n",
    "    vessels['length'] = vessels['length'].fillna(vessels['length_pred']).fillna(vessels['length'].mean())\n",
    "    vessels['depth'] = vessels['depth'].fillna(vessels['depth_pred']).fillna(vessels['depth'].mean())\n",
    "\n",
    "    #drop predictions\n",
    "    vessels = vessels.drop(['length_pred', 'depth_pred'], 1)\n",
    "\n",
    "    # Categorize length/depth\n",
    "    vessels['length'] = np.where(vessels['length'] < vessels['length'].quantile(0.25), 'small',\n",
    "                                    np.where(vessels['length'] < vessels['length'].quantile(0.5), 'medium',\n",
    "                                            np.where(vessels['length'] < vessels['length'].quantile(0.75), 'large', \n",
    "                                                     'very large')))\n",
    "\n",
    "    vessels['depth'] = np.where(vessels['depth'] < vessels['depth'].quantile(0.25), 'small',\n",
    "                                    np.where(vessels['depth'] < vessels['depth'].quantile(0.5), 'medium',\n",
    "                                            np.where(vessels['depth'] < vessels['depth'].quantile(0.75), 'large', \n",
    "                                                     'very large')))\n",
    "\n",
    "    #reorder columns\n",
    "    vessels = vessels[['mmsi', 'ship_type', 'speed', 'length', 'depth']]\n",
    "\n",
    "\n",
    "    print('new shape:', vessels.shape)\n",
    "    \n",
    "    return vessels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visits\n",
    "- Drop irrelevant variables\n",
    "- Only take one instance per entry/exit line; drop the variable\n",
    "- Add mid features\n",
    "- Clean up columns\n",
    "- Combine visits from one port to the same port\n",
    "- Add target features\n",
    "- Remove visits according to the steps defined in the report\n",
    "- Take log of travel duration/stay duration\n",
    "- Add previous port and the port before that\n",
    "- Drop vessels with invalid mmsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_visits():\n",
    "    visits = pd.read_csv('ais_port_visits.csv')\n",
    "    print('old shape:', visits.shape)\n",
    "    visits = visits.drop(['type', 'visit_uuid', 'latest_known_port', 'port_name', 'imo'], 1)\n",
    "    visits['exit_datetime'] = visits['datetime'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    visits = visits[visits['action'] == 'enter'].drop('action', 1)\n",
    "    print('shape after remvoving exit instances:', visits.shape)\n",
    "    \n",
    "    print(len(visits[visits['stay_duration'] == 0]), 'instances were dropped for having a stay duration of 0.')        \n",
    "    visits = visits[visits['stay_duration'] != 0]\n",
    "\n",
    "    #mid info\n",
    "    mids = pd.read_csv('mids.csv', header=None, names=['mid', 'iso2', 'mmsi_iso3', 'idk', 'name'])\n",
    "    mids = mids.drop(['iso2', 'idk', 'name'], 1)\n",
    "\n",
    "    visits['mid'] = visits['mmsi'].astype(str).str[:3].astype(int)\n",
    "    visits = pd.merge(visits, mids, how='left', on='mid')\n",
    "    visits = visits[visits['mmsi_iso3'].notnull()]\n",
    "    visits['mmsi_region'] = visits['mmsi'].astype(str).str[0].astype('category')\n",
    "\n",
    "    #clean up columns\n",
    "    visits = visits[['mmsi', 'mmsi_iso3', 'mmsi_region', 'port_index', 'port_lat', 'port_long', \n",
    "                     'distance_to_port', 'datetime', 'stay_duration', 'exit_datetime']]\n",
    "\n",
    "    visits.rename(columns={'datetime':'entry_datetime'}, inplace=True)\n",
    "    visits['entry_datetime'] = pd.to_datetime(visits['entry_datetime'])\n",
    "    visits['exit_datetime'] = pd.to_datetime(visits['exit_datetime'])\n",
    "    visits['stay_duration'] = (visits['exit_datetime'] - visits['entry_datetime']) / pd.Timedelta(hours=1)\n",
    "\n",
    "    #sort visits\n",
    "    visits = visits.sort_values(['mmsi', 'entry_datetime']).reset_index(drop=True)\n",
    "\n",
    "    #Create target variables\n",
    "    visits['target_entry_datetime'] = visits['entry_datetime'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    visits['target_travel_duration'] = visits['target_entry_datetime'].sub(visits['exit_datetime'], axis=0) / np.timedelta64(1, 'h')\n",
    "    visits['target_port_index'] = visits['port_index'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    \n",
    "    #save dss instances\n",
    "    missings = visits[visits['target_port_index'].isnull()]\n",
    "    \n",
    "    #remove instances where current port == target port\n",
    "    indices = visits[(visits['port_index'] == visits['target_port_index']) &\n",
    "                    (visits['target_travel_duration'] < 12)].index\n",
    "    visits.iloc[indices-1, 7] = pd.to_datetime(visits.iloc[indices, 7].values)\n",
    "    visits = visits.drop(indices).reset_index(drop=True)\n",
    "    print(len(indices), 'instances were dropped for returning to the same port in less than 12 hours.')\n",
    "    \n",
    "    \n",
    "    #Redefine target features/previous features\n",
    "    visits['target_port_index'] = visits['port_index'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    visits['target_entry_datetime'] = visits['entry_datetime'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    visits['stay_duration'] = (visits['exit_datetime'] - visits['entry_datetime']) / pd.Timedelta(hours=1)\n",
    "    visits['target_stay_duration'] = visits['stay_duration'].shift(-1).where(visits['mmsi'].eq(visits['mmsi'].shift(-1)))\n",
    "    visits['target_travel_duration'] = visits['target_entry_datetime'].sub(visits['exit_datetime'], axis=0) / np.timedelta64(1, 'h')\n",
    "    \n",
    "    \n",
    "    #remove visits according to the preprocessing steps described\n",
    "    pt_ports = pd.pivot_table(visits, values=['port_lat', 'port_long'], index='port_index', aggfunc='mean')\n",
    "    visits = pd.merge(visits, pt_ports.rename(columns={'port_lat':'target_port_lat',\n",
    "                                             'port_long': 'target_port_long'}),\n",
    "                how='left', left_on='target_port_index', right_index=True)\n",
    "    visits['distance'] = np.sqrt((visits['port_lat'] - visits['target_port_lat'])**2 + \n",
    "                                     (visits['port_long'] - visits['target_port_long'])**2)\n",
    "    visits['speed'] = (visits['distance']*111) / visits['target_travel_duration']\n",
    "    \n",
    "    print(len(visits[(visits['speed'] > 50)]), 'instances were dropped for having a speed > 50 kmph.')    \n",
    "    visits = visits[(visits['speed'] < 50)].reset_index(drop=True)\n",
    "    \n",
    "    indices = visits[(visits['speed'] < 0.1) & (visits['port_index'] != visits['target_port_index'])].index\n",
    "    print(len(indices), 'instances were dropped for having a speed < 0.1 kmph.')    \n",
    "    visits = visits.drop(indices)    \n",
    "    \n",
    "    visits = visits.drop(['speed', 'distance', 'target_port_lat', 'target_port_long'], 1)\n",
    "    \n",
    "    print(len(visits[(visits['target_travel_duration'] > 1250)]), 'instances were dropped for having a travel duration > 1250.')    \n",
    "    visits = visits[(visits['target_travel_duration'] < 1250)]    \n",
    "    \n",
    "    print(len(visits[(visits['target_travel_duration'] < 0.25)]), 'instances were dropped for having a travel duration < 0.1.')    \n",
    "    visits = visits[(visits['target_travel_duration'] > 0.25)] \n",
    "    \n",
    "    print(len(visits[(visits['target_stay_duration'] > visits['target_stay_duration'].quantile(0.99))]),\n",
    "          'instances were dropped for having a stay duration larger than the 99% percentile.')\n",
    "    visits = visits[(visits['target_stay_duration'] < visits['target_stay_duration'].quantile(0.99))]\n",
    "    \n",
    "    #take log of stay_duration/travel_duration\n",
    "    visits['stay_duration'] = np.log(visits['stay_duration'])\n",
    "    visits['target_stay_duration'] = np.log(visits['target_stay_duration'])\n",
    "    visits['target_travel_duration'] = np.log(visits['target_travel_duration'])\n",
    "    \n",
    "    #add dss instances again\n",
    "    visits = pd.concat([visits, missings])\n",
    "    visits = visits.sort_values(['mmsi', 'entry_datetime']).reset_index(drop=True)\n",
    "\n",
    "    #add previous port features\n",
    "    visits['previous_port_index'] = visits['port_index'].shift(1).where(visits['mmsi'].eq(visits['mmsi'].shift(1)))\n",
    "    visits['previous_distance_to_port'] = visits['distance_to_port'].shift(1).where(visits['mmsi'].eq(visits['mmsi'].shift(1)))\n",
    "    visits['previous_exit_datetime'] = visits['exit_datetime'].shift(1).where(visits['mmsi'].eq(visits['mmsi'].shift(1)))\n",
    "    visits['previous_entry_datetime'] = visits['entry_datetime'].shift(1).where(visits['mmsi'].eq(visits['mmsi'].shift(1)))\n",
    "    visits['previous_stay_duration'] = visits['stay_duration'].shift(1).where(visits['mmsi'].eq(visits['mmsi'].shift(1)))\n",
    "    visits['previous_travel_duration'] = visits['entry_datetime'].sub(visits['previous_exit_datetime'], axis=0) / np.timedelta64(1, 'h')\n",
    "    visits['previous_travel_duration'] = np.log(visits['previous_travel_duration'])\n",
    "    \n",
    "    #add prev2 port features\n",
    "    visits['prev2_port_index'] = visits['port_index'].shift(2).where(visits['mmsi'].eq(visits['mmsi'].shift(2)))\n",
    "    visits['prev2_distance_to_port'] = visits['distance_to_port'].shift(2).where(visits['mmsi'].eq(visits['mmsi'].shift(2)))\n",
    "    visits['prev2_exit_datetime'] = visits['exit_datetime'].shift(2).where(visits['mmsi'].eq(visits['mmsi'].shift(2)))\n",
    "    visits['prev2_stay_duration'] = visits['stay_duration'].shift(2).where(visits['mmsi'].eq(visits['mmsi'].shift(2)))\n",
    "    visits['prev2_travel_duration'] = visits['previous_entry_datetime'].sub(visits['prev2_exit_datetime'], axis=0) / np.timedelta64(1, 'h')\n",
    "    visits['prev2_travel_duration'] = np.log(visits['prev2_travel_duration'])    \n",
    "    \n",
    "    \n",
    "    #only take instances with a previous port\n",
    "    print(len(visits[((visits['previous_port_index'].isnull()) & \n",
    "                      (visits['target_port_index'].isnull()))]), 'instances were removed because they were singular.')\n",
    "    visits = visits[~((visits['previous_port_index'].isnull()) & \n",
    "                      (visits['target_port_index'].isnull()))]\n",
    "    \n",
    "    #only take mmsis with 9 digits\n",
    "    print(len(visits[visits['mmsi'] < 201000000]), 'instances were removed due to invalid mmsi.')\n",
    "    visits = visits[visits['mmsi'] >= 201000000].sort_values(['mmsi', 'entry_datetime']).set_index(['mmsi', 'port_index']).reset_index()\n",
    "    \n",
    "    print('new shape:', visits.shape)\n",
    "    \n",
    "    return visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nports')\n",
    "ports = prep_ports()\n",
    "print('\\nvessels')\n",
    "vessels = prep_vessels()\n",
    "print('\\nvisits')\n",
    "visits = prep_visits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits[['mmsi', 'port_index', 'port_lat',\n",
    "       'port_long', 'entry_datetime', 'stay_duration',\n",
    "       'exit_datetime', 'target_port_index', 'target_entry_datetime',\n",
    "       'target_travel_duration']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['target_travel_duration'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['target_stay_duration'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING\n",
    "- Features are created by using the historic visits\n",
    "- This is done for ports and vessels\n",
    "- Also the datetime features for the visits were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_port_feats(hist_visits, vessels):\n",
    "    hist_visits = pd.merge(hist_visits, vessels, how='left', on='mmsi')\n",
    "    hist_visits[['ship_type', 'speed', \n",
    "                 'length', 'depth']] = hist_visits[['ship_type', 'speed', \n",
    "                                                    'length', 'depth']].fillna('Unknown') \n",
    "    \n",
    "    port_features = pd.DataFrame(hist_visits['port_index'].unique(), columns=['port_index'])\n",
    "\n",
    "    #number of visits\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', values='mmsi', \n",
    "            aggfunc='count').reset_index().rename(columns={'mmsi':'n_visits'}), \n",
    "                             how='left', on='port_index')\n",
    "\n",
    "    #number of unique vessels\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', values='mmsi', \n",
    "            aggfunc='nunique').reset_index().rename(columns={'mmsi':'n_unique_vessels'}), \n",
    "                             how='left', on='port_index')\n",
    "    \n",
    "    #number of unique vessel origins\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', values='mmsi_iso3', \n",
    "            aggfunc='nunique').reset_index().rename(columns={'mmsi_iso3':'n_unique_vessel_origins'}), \n",
    "                             how='left', on='port_index')\n",
    "\n",
    "    #number of visits per vessel speed category\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', columns='speed', values='mmsi', \n",
    "            aggfunc='count').fillna(0).reset_index().rename(columns={'High': 'n_high_speed',\n",
    "                                                                    'Medium': 'n_medium_speed',\n",
    "                                                                    'Unknown': 'n_unknown_speed'}), \n",
    "                             how='left', on='port_index')\n",
    "\n",
    "    #number of visits per vessel type category\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', columns='ship_type', values='mmsi', \n",
    "            aggfunc='count').fillna(0).reset_index().rename(columns={'Chemical/Oil Tanker': 'n_Chemical/Oil Tanker',\n",
    "                                                                    'Container Ship': 'n_Container Ship',\n",
    "                                                                    'Crude Oil Tanker': 'n_Crude Oil Tanker',\n",
    "                                                                    'General Cargo Ship': 'n_General Cargo Ship',\n",
    "                                                                    'Tanker': 'n_Tanker',\n",
    "                                                                    'Unknown': 'n_Unknown_shiptype'}), \n",
    "                             how='left', on='port_index')\n",
    "    \n",
    "    #number of visits per vessel length category\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', columns='length', values='mmsi', \n",
    "                aggfunc='count').fillna(0).reset_index().rename(columns={'large': 'n_large_length',\n",
    "                                                                        'medium': 'n_medium_length',\n",
    "                                                                        'small': 'n_small_length',\n",
    "                                                                        'very large': 'n_very large_length',\n",
    "                                                                        'Unknown': 'n_Unknown_length'}), \n",
    "                             how='left', on='port_index')\n",
    "\n",
    "    #number of visits per vessel depth category\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index', columns='depth', values='mmsi', \n",
    "                aggfunc='count').fillna(0).reset_index().rename(columns={'large': 'n_large_depth',\n",
    "                                                                        'medium': 'n_medium_depth',\n",
    "                                                                        'small': 'n_small_depth',\n",
    "                                                                        'very large': 'n_very large_depth',\n",
    "                                                                        'Unknown': 'n_Unknown_depth'}), \n",
    "                             how='left', on='port_index')   \n",
    "    \n",
    "    #take percentages instead of counts, since counts are highly correlated\n",
    "    cols = ['n_unique_vessels', 'n_high_speed', 'n_medium_speed', \n",
    "            'n_Chemical/Oil Tanker', 'n_Container Ship', 'n_Crude Oil Tanker', \n",
    "            'n_General Cargo Ship', 'n_Tanker', 'n_large_length', \n",
    "            'n_medium_length', 'n_small_length', 'n_very large_length', \n",
    "            'n_large_depth', 'n_medium_depth', 'n_small_depth', 'n_very large_depth', 'n_Unknown_shiptype']\n",
    "    \n",
    "    port_features[cols] = port_features[cols].divide(port_features['n_visits'], axis=0)\n",
    "    \n",
    "    #Average stay duration/travel duration/distance to port\n",
    "    port_features = pd.merge(port_features, pd.pivot_table(hist_visits, index='port_index',\n",
    "                values=['stay_duration', 'distance_to_port', 'previous_travel_duration'], \n",
    "                aggfunc='mean').reset_index().rename(columns={'stay_duration': 'port_avg_stay_duration',\n",
    "                                                            'distance_to_port': 'port_avg_distance_to_port',\n",
    "                                                            'previous_travel_duration': 'port_avg_travel_duration'}), \n",
    "                             how='left', on='port_index')    \n",
    "    \n",
    "    port_features = port_features.drop(['n_unknown_speed', 'n_Unknown_length', 'n_Unknown_depth'], 1)\n",
    "    \n",
    "    return port_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_vessel_feats(hist_visits, ports):\n",
    "    hist_visits = pd.merge(hist_visits.drop(['port_lat', 'port_long'], 1), ports, how='left', on='port_index')\n",
    "        \n",
    "    vessel_features = pd.DataFrame(hist_visits['mmsi'].unique(), columns=['mmsi'])\n",
    "    \n",
    "    #number of visits\n",
    "    vessel_features = pd.merge(vessel_features, pd.pivot_table(hist_visits, index='mmsi', values='port_index', \n",
    "            aggfunc='count').reset_index().rename(columns={'port_index':'n_visits'}), how='left', on='mmsi')\n",
    "    \n",
    "    #number of unique ports visited\n",
    "    vessel_features = pd.merge(vessel_features, pd.pivot_table(hist_visits, index='mmsi', values='port_index', \n",
    "            aggfunc='nunique').reset_index().rename(columns={'port_index':'n_unique_ports'}), \n",
    "                               how='left', on='mmsi')\n",
    "    \n",
    "    #average travel/stay duration\n",
    "    vessel_features = pd.merge(vessel_features, pd.pivot_table(hist_visits, index='mmsi',\n",
    "                values=['stay_duration', 'distance_to_port', 'previous_travel_duration'], \n",
    "                aggfunc='mean').reset_index().rename(columns={'stay_duration': 'vessel_avg_stay_duration',\n",
    "                                                            'distance_to_port': 'vessel_avg_distance_to_port',\n",
    "                                                            'previous_travel_duration': 'vessel_avg_travel_duration'}), \n",
    "                             how='left', on='mmsi')\n",
    "    \n",
    "    #favorite port\n",
    "    vessel_features = pd.merge(vessel_features, pd.pivot_table(hist_visits, columns='port_index', values='mmsi_region', \n",
    "                index='mmsi', aggfunc='count').idxmax(axis=1).reset_index().rename(columns={0: 'vessel_fav_port_index'}),\n",
    "                               how='left', on='mmsi')\n",
    "    \n",
    "    #average coordinates\n",
    "    pt = pd.pivot_table(hist_visits, index='mmsi', values=['port_lat', 'port_long'], \n",
    "            aggfunc=['mean', 'std'])\n",
    "    \n",
    "    pt.columns = ['_'.join((str(j), str(k))) for j, k in pt.columns]\n",
    "    vessel_features = pd.merge(vessel_features, pt.reset_index(), how='left', on='mmsi')\n",
    "    \n",
    "    return vessel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_visits(visits):\n",
    "    #add some datetime features\n",
    "    visits['previous_exit_hour'] = pd.to_datetime(visits['previous_exit_datetime']).dt.hour.astype('category')\n",
    "    visits['previous_exit_month'] = pd.to_datetime(visits['previous_exit_datetime']).dt.month.astype('category')\n",
    "    visits['previous_exit_weekday'] = pd.to_datetime(visits['previous_exit_datetime']).dt.weekday.astype('category')\n",
    "    visits['previous_exit_quarter'] = pd.to_datetime(visits['previous_exit_datetime']).dt.quarter.astype('category')\n",
    "    visits['previous_exit_season'] = (pd.to_datetime(visits['previous_exit_datetime']).dt.month %12 // 3 + 1).astype('category')\n",
    "\n",
    "    visits['exit_hour'] = pd.to_datetime(visits['exit_datetime']).dt.hour.astype('category')\n",
    "    visits['exit_month'] = pd.to_datetime(visits['exit_datetime']).dt.month.astype('category')\n",
    "    visits['exit_weekday'] = pd.to_datetime(visits['exit_datetime']).dt.weekday.astype('category')\n",
    "    visits['exit_quarter'] = pd.to_datetime(visits['exit_datetime']).dt.quarter.astype('category')\n",
    "    visits['exit_season'] = (pd.to_datetime(visits['exit_datetime']).dt.month %12 // 3 + 1).astype('category')\n",
    "\n",
    "    visits.drop('previous_exit_datetime', 1)\n",
    "    \n",
    "    return visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split off historic data and DSS data\n",
    "- First add datetime features to the visit dataset\n",
    "- Select the oldest data for the historic dataset\n",
    "- Split the dataset\n",
    "- Look at some stats of the historic dataset compared to the train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = features_visits(visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = visits[visits['target_port_index'].notnull()][['mmsi', 'port_index', 'entry_datetime',\n",
    "                'previous_port_index', 'prev2_port_index']]\n",
    "\n",
    "\n",
    "# select visits without a previous port\n",
    "hist_indices = indices[indices['prev2_port_index'].isnull()].index.values\n",
    "\n",
    "# select oldest 50% of the data\n",
    "indices = indices.sort_values(['entry_datetime'], ascending=True)\n",
    "\n",
    "hist_indices = np.append(hist_indices, indices.head(int(len(indices)*0.5)).index.values)\n",
    "\n",
    "# drop duplicates\n",
    "hist_indices = pd.Series(hist_indices).unique()\n",
    "\n",
    "len(hist_indices) / len(visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_visits = visits.iloc[hist_indices]\n",
    "train_visits = visits.loc[~visits.index.isin(hist_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_visits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_visits['port_index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visits['port_index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_visits['mmsi'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visits['mmsi'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ports dataset with correct coordinates and new features\n",
    "- Find correct coordinates from visit dataset\n",
    "- Add them to ports\n",
    "- Create new features\n",
    "- Add new features to ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = prep_ports()\n",
    "pt_ports = pd.pivot_table(visits, values=['port_lat', 'port_long'], index='port_index', aggfunc='mean')\n",
    "ports = pd.merge(ports, pt_ports.reset_index(), how='right', on='port_index')\n",
    "\n",
    "port_features = historic_port_feats(hist_visits, vessels)\n",
    "ports = pd.merge(ports, port_features, how='left', on='port_index')\n",
    "\n",
    "ports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update vessel dataset with new features\n",
    "- Add vessel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_features = historic_vessel_feats(hist_visits, ports)\n",
    "\n",
    "vessels = pd.merge(vessels, vessel_features, how='outer', on='mmsi')\n",
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "- Merge the datasets\n",
    "- Also add the port features for the previous/previous previous/and vessel favorite ports\n",
    "- Add seadistances between countries based on the CERDI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(vessels, train_visits, how='right', on='mmsi')\n",
    "df = pd.merge(df, ports.drop(['port_lat', 'port_long'], 1), how='left', on='port_index')\n",
    "\n",
    "og_ports_cols = ports.columns\n",
    "ports.columns = ['previous_' + column for column in og_ports_cols]\n",
    "df = pd.merge(df, ports, how='left', on='previous_port_index')\n",
    "\n",
    "ports.columns = ['prev2_' + column for column in og_ports_cols]\n",
    "df = pd.merge(df, ports, how='left', on='prev2_port_index')\n",
    "\n",
    "ports.columns = ['vessel_fav_' + column for column in og_ports_cols]\n",
    "df = pd.merge(df, ports, how='left', on='vessel_fav_port_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.read_excel('CERDI.xlsx')\n",
    "distances = distances.set_index(['iso1', 'iso2'])\n",
    "\n",
    "df = pd.merge(df, distances[['seadistance']].rename(columns={'seadistance': 'previous_seadistance'}), \n",
    "              how='left', left_on=['previous_iso3', 'iso3'], right_index=True)\n",
    "df['previous_seadistance'] = df['previous_seadistance'].fillna(0)\n",
    "\n",
    "df = pd.merge(df, distances[['seadistance']].rename(columns={'seadistance': 'prev2_cur_seadistance'}), \n",
    "              how='left', left_on=['prev2_iso3', 'iso3'], right_index=True)\n",
    "df['prev2_cur_seadistance'] = df['prev2_cur_seadistance'].fillna(0)\n",
    "\n",
    "df = pd.merge(df, distances[['seadistance']].rename(columns={'seadistance': 'prev2_prev_seadistance'}), \n",
    "              how='left', left_on=['prev2_iso3', 'previous_iso3'], right_index=True)\n",
    "df['prev2_prev_seadistance'] = df['prev2_prev_seadistance'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save datasets\n",
    "- train_visits contains all the information for the models\n",
    "    - Note that it also contains the visits for the dss (i.e. train_visits[train_visits['target_port_index'].isnull()])\n",
    "    - We can only predict the stay duration/travel duration if we have predicted a next port\n",
    "- hist_visits contains the visits based on which the historic port features were computed\n",
    "- ports contains all the port information\n",
    "- vessels contains all the vessel information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports.columns = og_ports_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports.to_parquet('ports_prep.parquet.gzip',\n",
    "              compression='gzip', index=False)\n",
    "vessels.to_parquet('vessels_prep.parquet.gzip',\n",
    "              compression='gzip', index=False)\n",
    "df.to_parquet('train_visits.parquet.gzip',\n",
    "              compression='gzip', index=False)\n",
    "hist_visits.to_parquet('hist_visits.parquet.gzip',\n",
    "              compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hist_visits.shape)\n",
    "hist_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vessels.shape)\n",
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ports.shape)\n",
    "ports.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
